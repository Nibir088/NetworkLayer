{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ContextSummarization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nibir088/NetworkLayer/blob/master/ContextSummarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "JlV1LILuXdvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "fe5d88d4-5a62-41cf-abf4-2bd5bb6821f2"
      },
      "cell_type": "code",
      "source": [
        "#ML Project: Bangla Context Summarization\n",
        "#Project Member : Dibyendu Das Pranto, Shuvashis Kar Antu, Nibir Chandra Mandal\n",
        "\n",
        "#This File is for data preprocessing\n",
        "\n",
        "\n",
        "#this part is created by Nibir\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "if __name__=='__main__':\n",
        "    #dataset imprting Test-1 \n",
        "    datafile='articles1.csv'\n",
        "    dataset=pd.read_csv(datafile)   \n",
        "    context=dataset['content']\n",
        "    \n",
        "    \n",
        "    \n",
        "    context=context[:10]\n",
        "    \n",
        "    #dataset importing test Ends     *******context(List) holds article*********** \n",
        "    \n",
        "    #NLP parts\n",
        "    articles=[]\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    stopWords.add('could')\n",
        "    ps = PorterStemmer()\n",
        "    \n",
        "    \n",
        "    for article in context:\n",
        "        temp = sent_tokenize(article)\n",
        "        sentences=[]\n",
        "        for each_sentence in temp:\n",
        "            each_sentence=word_tokenize(each_sentence)\n",
        "            sentence=[]\n",
        "            for each_word in each_sentence:\n",
        "                word=ps.stem(each_word)\n",
        "                if word not in stopWords:\n",
        "                    sentence.append(word)\n",
        "            sentences.append(sentence)\n",
        "        \n",
        "        articles.append(sentences)\n",
        "    \n",
        "    sentences_vect=[e for sl in articles for e in sl] \n",
        "    \n",
        "    dictionary = corpora.Dictionary(sentences_vect)\n",
        "    #print(dictionary.id2token['come'])\n",
        "    "
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-f26c8b3b54eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'come'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'come'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ZvtV8sowlGzq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3381e3e7-34f3-4f01-cd30-805b75e3f8e4"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#sentences = sent_tokenize(\"Given a dataset containing a sequence of sentences, the decoder is expected to generate the previous and next sentences, word by word. The encoder-decoder network is trained to minimize the sentence reconstruction loss, and in doing so, the encoder learns to generate vector representations which encode enough information for the decoder, so that it can generate neighboring sentences.\")\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "dBwjeIHqaTPs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('40k-bangla-newspaper-article.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hMFHdRyXxhPE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d zshujon/40k-bangla-newspaper-article"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jat_l01y0-qI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fCqPkvRy1GZp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ML Project: Bangla Context Summarization\n",
        "#Project Member : Dibyendu Das Pranto, Shuvashis Kar Antu, Nibir Chandra Mandal\n",
        "\n",
        "#This File is for data preprocessing\n",
        "\n",
        "\n",
        "#this part is created by Nibir\n",
        "import numpy as np\n",
        "import pickle as p\n",
        "import spacy\n",
        "import bn_core_news_sm\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "    datafile='40k_bangla_newspaper_article.p'\n",
        "    dataset=p.load(open(datafile,'rb'))   #dataset loading. Dataset contains articles in a dictionary. \n",
        "    context=[]\n",
        "    for article in dataset:\n",
        "        context.append(article['content'])\n",
        "    print(context[0])\n",
        "        \n",
        "    \n",
        "    \n",
        "    #dataset importing test-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I8bDimQA1pJm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "b2304f53-b13d-4eb1-a26e-25587be7a4c4"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en')\n",
        "doc=nlp('I am Nibir')\n",
        "for token in doc:\n",
        "    print(token.idx,' ',token.lemma_)\n",
        "\n",
        "doc=nlp('Karim is human. I like him')\n",
        "for token in doc:\n",
        "    print(token.idx,' ',token.lemma_)\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0   -PRON-\n",
            "2   be\n",
            "5   nibir\n",
            "0   karim\n",
            "6   be\n",
            "9   human\n",
            "14   .\n",
            "16   -PRON-\n",
            "18   like\n",
            "23   -PRON-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M0B22QV_798Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1754
        },
        "outputId": "e2fe005b-fe5b-443c-aece-10ca2d910d51"
      },
      "cell_type": "code",
      "source": [
        "import gensim "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.014531  -0.071761  -0.59626   -0.1601     0.012435   0.10506\n",
            "  0.039014   0.070334  -0.22503    2.4932    -0.36484   -0.14024\n",
            "  0.31678    0.060765  -0.48319   -0.098549  -0.14103    1.0628\n",
            " -0.36518    0.095546   0.50101    0.22637    0.15889    0.091197\n",
            " -0.36923    0.14988   -0.139     -0.21428    0.67552   -0.29393\n",
            " -0.043688   0.41393   -0.26467   -0.085296   0.27927    0.18549\n",
            "  0.22137    0.016852  -0.12807   -0.46307   -0.41046    0.22182\n",
            " -0.01718   -0.066614   0.1303     0.063694  -0.1937     0.10319\n",
            " -0.075827   0.044023  -0.0532     0.019465   0.65215   -0.15037\n",
            "  0.48806   -0.093592   0.19344   -0.13372    0.22679   -0.058024\n",
            "  0.046199  -0.15737   -0.26732    0.44735    0.39518   -0.22501\n",
            "  0.10479   -0.059828   0.3292     0.16094    0.34499    0.13399\n",
            "  0.21925    0.10909    0.48094    0.24972    0.20757   -0.44005\n",
            "  0.25757    0.41252    0.12334   -0.14352   -0.27867   -0.24367\n",
            " -0.23289    0.0082524  0.091486  -0.50635    0.47561   -0.03489\n",
            " -0.11767   -0.026811   0.039398   0.29118    0.23716    0.27771\n",
            "  0.22877    0.058699   0.23151   -0.16729   -0.40305    0.54796\n",
            " -0.026562  -0.086863   0.25399   -0.92828   -0.096341  -0.29277\n",
            " -0.10595    0.10353    0.024039  -0.27735    0.067963  -0.17551\n",
            "  0.46432   -0.47883    0.040431  -0.28839   -0.20093    0.22343\n",
            "  0.25773   -0.53181   -0.14683   -0.12345    0.27601    0.11329\n",
            "  0.11818   -0.085023   0.25846   -0.080644   0.33507    0.26379\n",
            " -0.35468    0.13993    0.18758    0.12955   -0.22897   -0.14231\n",
            "  0.026698   0.06458   -1.841     -0.038086  -0.035655  -0.074406\n",
            "  0.16073    0.025417   0.083438   0.18088   -0.099236  -0.10634\n",
            "  0.33865    0.10909    0.17294    0.3497    -0.059236   0.088815\n",
            " -0.18059   -0.50287   -0.043353  -0.042204  -0.027376   0.18659\n",
            " -0.5949     0.25639    0.12299    0.15006   -0.05949   -0.053698\n",
            "  0.29875    0.17149   -0.21302    0.013318   0.2771    -0.44522\n",
            "  0.23034    0.45266   -0.04251    0.40827    0.20466    0.20581\n",
            "  0.042323  -0.014975  -0.20133   -0.076192   0.1752     0.10449\n",
            "  0.058652  -0.11021    0.35654    0.21623    0.41617    0.090905\n",
            " -0.43823   -0.55482    0.04999   -0.19691   -0.36534   -0.17466\n",
            "  0.4757     0.62248   -0.10407    0.10593    0.19015   -0.49602\n",
            "  0.072286   0.095801   0.082364  -0.13075    0.39061    0.25836\n",
            "  0.021462  -0.068737  -0.082929  -0.14876   -0.062896   0.17199\n",
            " -0.08708    0.18082   -0.34376    0.16352    0.26193    0.082947\n",
            " -0.14184   -0.14149    0.086732   0.44812   -0.13074    0.064635\n",
            " -0.24978   -0.32167   -0.53034   -0.094244   0.29788    0.37628\n",
            " -0.34317   -0.081779   0.32126   -0.55451   -0.29675    0.22058\n",
            "  0.34847    0.25047    0.13904    0.04262    0.0080664 -0.40149\n",
            " -0.23168   -0.063996  -0.48293    0.67323    0.14676   -0.32935\n",
            " -0.68048   -0.15401   -0.28397    0.25797   -0.30016   -0.090222\n",
            "  0.26642    0.068262   0.055926   0.36799   -0.011665   0.23247\n",
            "  0.098703   0.37469    0.025494   0.050794   0.43956    0.53834\n",
            "  0.13445   -0.39782    0.1703    -0.16837   -0.058985   0.2257\n",
            " -0.078279  -0.28434   -0.11361    0.3328    -0.25523   -0.011575\n",
            "  0.087378  -0.056628  -0.33276   -0.32029    0.22145   -0.27901\n",
            "  0.12005    0.15651   -0.45846    0.083874  -0.44807   -0.066308\n",
            "  0.257      0.13312   -0.11189   -0.12443    0.63754    0.12262  ]\n",
            "[-1.3563e-01  3.3217e-01 -3.6020e-01 -5.8787e-02  6.4158e-02  1.6615e-01\n",
            "  3.3234e-03 -3.2969e-01  7.5679e-02  2.5391e+00 -1.5764e-01 -2.1620e-01\n",
            " -1.4259e-01  1.1177e-01 -2.9931e-01 -1.9899e-01 -2.1823e-01  1.0180e+00\n",
            " -2.3597e-01 -1.4919e-02  5.0167e-02  7.4423e-02 -5.0534e-02 -2.6093e-01\n",
            " -1.6593e-01  2.7982e-01 -3.5979e-02 -2.2388e-01  2.3900e-01 -1.9692e-01\n",
            " -3.0998e-01  2.5846e-01 -5.8236e-02 -1.9219e-02  8.8768e-02  1.0352e-01\n",
            "  1.5960e-01 -2.5218e-01 -1.3673e-01 -3.5658e-01  1.2263e-01  2.3931e-01\n",
            "  1.7654e-01  1.6496e-02  1.9253e-01  6.8153e-02 -1.6250e-01  3.2560e-02\n",
            " -5.1659e-01  2.1811e-01 -3.3340e-02  1.1580e-01  6.9572e-02 -1.9000e-01\n",
            "  1.5414e-01 -1.1342e-01  2.1076e-01 -2.5120e-01 -5.4189e-02 -3.5678e-01\n",
            "  3.7694e-02 -1.0790e-01  9.1845e-02  3.3446e-01  3.2539e-01 -6.5791e-01\n",
            " -2.6146e-01  1.4743e-01  2.1868e-01 -6.4628e-02 -1.9586e-02 -3.2893e-02\n",
            "  6.9328e-02 -1.7445e-01 -1.6362e-01  5.6576e-01 -1.6877e-01 -2.1861e-01\n",
            "  2.0575e-01  4.4746e-01  2.1607e-01 -2.5768e-01 -1.9970e-02  1.3338e-01\n",
            " -7.7638e-02 -4.5196e-01  3.3278e-01 -7.1140e-01  4.6350e-01 -1.2387e-01\n",
            " -3.0099e-01  2.2850e-01 -2.1420e-01  1.2092e-02  3.6135e-01 -1.1658e-01\n",
            " -6.8456e-02  6.0633e-02  1.2371e-01  3.0123e-01  1.6337e-01  2.0130e-01\n",
            " -7.5338e-03 -1.9371e-03  1.3258e-01 -8.3388e-01 -1.4257e-02  1.9819e-01\n",
            " -2.8765e-01 -1.5075e-01  5.9529e-03 -1.5831e-02 -1.9755e-01 -7.8979e-02\n",
            "  1.9917e-01 -5.3273e-02  1.9687e-01 -1.4584e-01 -1.1137e-02  4.9415e-02\n",
            " -2.0387e-01 -2.5699e-01  5.1327e-02 -6.7651e-03 -1.0436e-01  4.5134e-02\n",
            " -1.3851e-01 -6.0661e-01  1.7360e-01  8.5491e-03 -7.0439e-02  6.1775e-02\n",
            "  1.8558e-01  1.3694e-01  1.5688e-01 -3.6744e-03 -9.7024e-02 -2.8351e-01\n",
            " -3.7689e-02 -2.8577e-01 -1.3984e+00  2.1005e-02  4.7446e-01  2.7114e-01\n",
            " -9.4809e-02  5.9627e-02  1.5202e-01  4.1721e-02  4.8887e-02  1.0949e-01\n",
            "  2.4958e-01  2.0470e-01 -8.0497e-03  2.8010e-01  1.7983e-01 -3.2344e-01\n",
            " -1.5282e-01 -2.7350e-01 -2.1715e-01  1.2271e-01 -4.1326e-01  3.0648e-01\n",
            " -3.0906e-01  1.7056e-01  9.4023e-02 -1.1906e-01 -5.2241e-02 -1.4350e-01\n",
            "  1.2782e-01 -1.3185e-01 -1.0039e-01 -1.4454e-01  1.2298e-01 -3.7010e-01\n",
            "  1.8042e-01  4.6709e-01  2.7063e-02  4.6578e-02  5.0633e-02  1.4060e-01\n",
            "  1.7259e-01 -4.8847e-02 -2.2821e-01 -1.3354e-01 -3.3314e-03  3.0340e-01\n",
            "  1.3573e-01  1.1175e-03 -4.5207e-02  3.2402e-02 -1.1882e-01 -8.1540e-03\n",
            " -5.0982e-01 -7.2078e-02  1.0493e-01  2.0291e-02 -4.0100e-01 -2.7546e-01\n",
            "  5.0595e-01  6.1059e-01 -3.2399e-01  7.5294e-02 -3.9479e-01 -3.3793e-01\n",
            " -9.0266e-02  1.7546e-01  1.0469e-01 -1.4773e-01  2.4589e-01  2.2966e-01\n",
            " -2.4530e-01  3.5371e-02  6.5276e-02 -2.2209e-01  1.1157e-02  5.1737e-02\n",
            " -1.8568e-01  1.2741e-01 -1.9354e-01 -1.1380e-01  1.7450e-01  1.8861e-01\n",
            " -9.3585e-02 -1.5734e-01  2.8466e-01  3.2489e-01  3.0787e-01 -5.0206e-02\n",
            "  1.7135e-01  6.6522e-02 -3.4438e-01 -1.4255e-01  4.1428e-01  2.7162e-01\n",
            " -1.9451e-01 -2.3710e-02  4.6642e-01 -2.7294e-01  8.0506e-02  2.2325e-01\n",
            "  1.5066e-01  1.7210e-01 -3.9996e-02 -1.0151e-01  9.4544e-02 -6.0565e-01\n",
            "  9.9110e-02 -2.0519e-01 -6.3281e-02  2.3256e-01  3.9219e-01 -3.2049e-01\n",
            " -4.3040e-02 -3.8520e-01 -4.0593e-01  7.8540e-02  2.1652e-02 -8.2311e-02\n",
            "  5.6336e-01 -7.2243e-02  2.6582e-01  1.4685e-01  2.7719e-02 -4.6186e-03\n",
            "  3.7939e-01  4.0162e-01 -2.8569e-01 -1.1752e-02  9.2523e-01  2.7730e-01\n",
            "  3.7206e-01  1.1402e-02  2.7072e-01  1.3099e-02  1.1574e-01  7.9911e-02\n",
            " -1.3441e-01 -1.0497e-01  1.2243e-01  1.9787e-01 -1.3612e-01  1.5882e-01\n",
            "  2.9287e-01 -1.2484e-01 -4.6008e-01 -1.2185e-01 -2.9239e-02  8.2938e-02\n",
            "  5.8937e-01 -3.0290e-01 -7.9546e-02 -6.5184e-03 -1.8983e-01  6.4747e-02\n",
            "  1.5852e-01  2.9398e-02 -1.5525e-01 -1.7296e-01  2.1676e-01  2.2206e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}